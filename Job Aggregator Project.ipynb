{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f08482-37c9-497d-ac04-a2257af14b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# BRONZE LAYER - Ingesting data to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22ecf05-3782-4f73-bd68-d2961c5b67a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests,json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#defining the Spark Session\n",
    "spark=SparkSession.builder.appName(\"JobEL_Bronze\").getOrCreate()\n",
    "\n",
    "#defining the Api key\n",
    "API_KEY=\"Shaj1YJ8eNVLWZpkStPamydt\"\n",
    "\n",
    "roles=[\"Data Engineer\",\"ETL Developer\",\"Spark Engineer\",\"Python Developer\",\"Data Analyst\"]\n",
    "location=\"India\"\n",
    "all_jobs=[]\n",
    "for role in roles:\n",
    "  params = {\n",
    "  \"engine\": \"google_jobs\",\n",
    "  \"q\":role,\n",
    "  \"location\":location,\n",
    "  \"api_key\": API_KEY\n",
    "  }\n",
    "  url = \"https://www.searchapi.io/api/v1/search\"\n",
    "  response = requests.get(url, params=params)\n",
    "  jobs=response.json().get(\"jobs\",[])\n",
    "  for job in jobs:\n",
    "      job[\"search_role\"]=role\n",
    "  all_jobs.extend(jobs)\n",
    "all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690123c7-30bd-458b-98bb-a89cd0b738f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#save json as delta table\n",
    "df=spark.createDataFrame(all_jobs)\n",
    "df.write.mode(\"append\").format(\"delta\").saveAsTable(\"jobs_bronze\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452c034a-6b80-4d9c-bba7-68567d958c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SILVER LAYER - Cleaning and structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "471f2367-e245-4d55-addf-653da97553dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#cleaning and selecting required columns\n",
    "df_silver=df.selectExpr(\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"company_name\",\n",
    "    \"location\",\n",
    "    \"detected_extensions.posted_at as posted_at\",\n",
    "    \"search_role\"\n",
    ").dropna(subset=[\"title\",\"company_name\",\"location\"])\n",
    "df_silver = df_silver.dropna(subset=[\"posted_at\"])\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38054b90-696d-431a-888c-d2fe70526a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, upper, col\n",
    "df_silver=df_silver.withColumn(\"company_name\",trim(upper(col(\"company_name\"))))\n",
    "display(df_silver)\n",
    "df_silver.write.mode(\"append\").format(\"delta\").saveAsTable(\"jobs_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b60370f-38f5-4f5b-b5b9-d55a1d80d7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GOLD Layer -- Generate KPI tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcaf635-5f04-402d-a8f9-9387557e36fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "df_gold=df_silver\n",
    "\n",
    "#KPI 1 - Top Companies\n",
    "top_companies= df_gold.groupBy(\"company_name\").count().orderBy(col(\"count\").desc())\n",
    "display(top_companies)\n",
    "\n",
    "#KPI 2 - Jobs By City\n",
    "display(df_gold.groupBy(\"location\").agg(count('*').alias(\"job_count\")).orderBy(col(\"job_count\").desc()))\n",
    "\n",
    "df_gold.write.mode(\"append\").format(\"delta\").saveAsTable(\"JobAggregator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb92209-7741-444e-b91a-708f14b54bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert dataframe to pandas\n",
    "import pandas as pdf\n",
    "\n",
    "pdf= df_gold.orderBy(\"posted_at\",ascending=True).limit(10).toPandas()\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2132fbea-47ae-44ca-aea0-c2b684cdc073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(pdf['company_name'][::-1], pdf['title'][::-1], color='skyblue')\n",
    "plt.xlabel('Number of jobs')\n",
    "plt.ylabel('Top Companies Hiring')\n",
    "plt.grid(True,axis='x',linestyle='--',alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Job Aggregator Project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
